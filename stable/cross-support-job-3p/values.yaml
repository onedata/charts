# Default values for cross-support-job
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

image: onedata/rest-cli:21.02.7
imagePullPolicy: IfNotPresent

wait_for:
  image: groundnuty/k8s-wait-for:v2.0
  imagePullPolicy: IfNotPresent

# Specifies if users job should wait for release to start
wait_for_release:
  enabled: true

# Specifies if groups job should wait for users job to end
wait_for_users:
  enabled: true

# Specifies if groups-join job should wait for groups job to end
wait_for_groups:
  enabled: true

# Specifies if spaces job should wait for groups-join job to end
wait_for_groups_join:
  enabled: true

# Specifies if supports job should wait for luma service to start
wait_for_luma:
  enabled: true

# Specifies if luma job should wait for spaces job to end
wait_for_spaces:
  enabled: true

# You can enable automatic deployment od the environment
# that this chart initializes
onedata-3p:
  enabled: true
  posix_enabled: &posix_enabled true
  s3_enabled: &s3_enabled true
  ceph_enabled: &ceph_enabled true
  nfs_enabled: &nfs_enabled true
  gluster_enabled: &gluster_enabled true
  swift_enabled: &swift_enabled false
  webdav_enabled: &webdav_enabled false
  dcache_enabled: &dcache_enabled false

  token_dispenser_enabled: &token_dispenser_enabled true

  oneprovider-krakow:
    posix:
      enabled: *posix_enabled
    volume-s3:
      enabled: *s3_enabled
    volume-ceph:
      enabled: *ceph_enabled
    volume-nfs:
      enabled: *nfs_enabled
    volume-gluster:
      enabled: *gluster_enabled

  oneprovider-paris:
    posix:
      enabled: *posix_enabled
    volume-s3:
      enabled: *s3_enabled
    volume-ceph:
      enabled: *ceph_enabled
    volume-nfs:
      enabled: *nfs_enabled
    volume-gluster:
      enabled: *gluster_enabled

  oneprovider-lisbon:
    posix:
      enabled: *posix_enabled
    volume-s3:
      enabled: *s3_enabled
    volume-ceph:
      enabled: *ceph_enabled
    volume-nfs:
      enabled: *nfs_enabled
    volume-gluster:
      enabled: *gluster_enabled

# Those variables are the unfortunate necessity of an issue discussed here:
# https://github.com/kubernetes/helm/issues/2479]
token-dispenser:
  enabled: *token_dispenser_enabled
volume-s3:
  enabled: *s3_enabled
volume-ceph:
  enabled: *ceph_enabled
volume-nfs:
  enabled: *nfs_enabled
volume-gluster:
  enabled: *gluster_enabled


# Or manually specify a helm release name of the already
# deployed environment
# releaseName: dev

# Run luma configuration job
lumaJobEnabled: false

# Set to [] to disable group creation job
groups: []

# Set to {} to disable user creation job
users:
  - &user_joe
    name: joe
    firstName: Joe
    lastName: Morgan
    idps:
      onepanel:
        enabled: true
        type: regular
        mode: config
    password: password
    email: joe@example.com
    oneclient: [ krakow ]

# Set to {} to disable space creation
# and support configuration jobs
spaces:
  - name: "krk-3"
    user: *user_joe
    supports:
      - provider: "krakow"
        storage_name: "ceph"
        size: '1000000000'
  - name: "par-n"
    user: *user_joe
    supports:
      - provider: "paris"
        storage_name: "nfs"
        size: '1000000000'
  - name: "lis-3"
    user: *user_joe
    supports:
      - provider: "lisbon"
        storage_name: "s3"
        size: '1000000000'
  - name: "krk-n-par-3"
    user: *user_joe
    supports:
      - provider: "krakow"
        storage_name: "nfs"
        size: '1000000000'
      - provider: "paris"
        storage_name: "s3"
        size: '1000000000'
  - name: "krk-3-lis-c"
    user: *user_joe
    supports:
      - provider: "krakow"
        storage_name: "s3"
        size: '1000000000'
      - provider: "lisbon"
        storage_name: "ceph"
        size: '1000000000'
  - name: "par-n-lis-c"
    user: *user_joe
    supports:
      - provider: "paris"
        storage_name: "nfs"
        size: '1000000000'
      - provider: "lisbon"
        storage_name: "ceph"
        size: '1000000000'
  - name: "krk-3-par-c-lis-n"
    user: *user_joe
    supports:
      - provider: "krakow"
        storage_name: "s3"
        size: '1000000000'
      - provider: "paris"
        storage_name: "ceph"
        size: '1000000000'
      - provider: "lisbon"
        storage_name: "nfs"
        size: '1000000000'
  - name: "krk-3-par-c-lis-n"
    user: *user_joe
    supports:
      - provider: "krakow"
        storage_name: "s3"
        size: '1000000000'
      - provider: "paris"
        storage_name: "ceph"
        size: '1000000000'
      - provider: "lisbon"
        storage_name: "nfs"
        size: '1000000000'
  - name: "krk-g"
    user: *user_joe
    supports:
      - provider: "krakow"
        storage_name: "gluster"
        size: '1000000000'
  - name: "krk-g-par-3"
    user: *user_joe
    supports:
      - provider: "krakow"
        storage_name: "gluster"
        size: '1000000000'
      - provider: "paris"
        storage_name: "ceph"
        size: '1000000000'

# The generalization of nodeSelector.
# Allows for more fine grained controls over which
# nodes are selected by a kubernetes scheduler
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

# List of taints which are tolerated by the pods
# when nodes are selected by a kubernetes scheduler
# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []

# Specify a map of key-value pairs. For the pod
# to be eligible to run on a node, the node
# must have each of the indicated key-value pairs as labels
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}