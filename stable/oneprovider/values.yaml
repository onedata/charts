# Default values for oneprovider.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

image: onedata/oneprovider:20.02.12
imagePullPolicy: IfNotPresent
serviceType: ClusterIP

onedata_cli:
  image: onedata/rest-cli:20.02.12
  imagePullPolicy: IfNotPresent

wait_for:
  image: groundnuty/k8s-wait-for:v2.0
  imagePullPolicy: IfNotPresent

# Specifies if ready check job should be created.
oneprovider_ready_check:
  enabled: true

# Wait for Onezone service to be ready before starting.
wait-for-onezone:
  enabled: true

# To register to Onezone, Oneprovider needs registration token
onezone-registration-token:
  # Automatically acquire provider registration token prior to starting onezone
  enabled: true
  # Or provide a token manually
  value: ""

# When running multiple copies of oneprovider as a part of a helm release
# each of oneprovider charts may be give a suffix, so that service names
# do not collide. This is propagated to all oneprovider chart dependencies.
suffix: &providerSuffix ''

# An address of Onezone service (mandatory)
# support for multiple scenarios, see examples bellow
onezone_service_url: &onezone_service_url
  # 1. no address is needed
  # the assumption is made that onezone is part of the same helm release
  # the disableSuffix flag, prevents suffix to be appended to onezone service name
  type: auto-generate
  disableSuffix: true

  # 2. a full address of an onezone (without http(s))
  # note that external onezones will not be able to communicate to k8s deployed oneprovider
  # type: http
  # address: beta.onedata.org

  # 3. only service name is sufficient, the namespace parameter is optional
  # type: k8s-service
  # service_name: rc14-onezone
  # namespace: default

# oneprovider name, if empty defaults to a chart name
name: ''

# Resources requested by oneprovider
cpu: 1.5
memory: 4Gi

# Coordinates of a Oneprovider on a onezone map
geoLatitude: 55.333
geoLongitude: 55.333

# Oneprovider admin email
adminEmail: "getting-started@onedata.org"

# Log level of the processes in the container
log_level: "info"

# Prevent oneprovider container to exit when container entrypoint fails
onepanel_debug_mode: false

# Enable loading oneprovider configuration from ONEPROVIDER_CONFIG env variable
onepanel_batch_mode_enabled: true

# If enabled, a new web cert will be generated with CN matching the
#   ONEPANEL_GENERATED_CERT_DOMAIN and signed by OnedataTestWebServerCa
# NOTE: The generation will be performed upon every startup, any
#   existing certs will be backed up and placed in the same directory.
# WARNING: This functionality is devised for test purposes and must not
#   be used in production.
onepanel_generate_test_web_cert: true

# The generated test web cert will be issued for below domain.
onepanel_generated_cert_domain: ""

# If enabled, onepanel will trust any server that has a cert signed by
#   the OnedataTestWebServerCa.
# WARNING: This functionality is devised for test purposes and must not
#   be used in production.
onepanel_trust_test_ca: true

# When true, all GUIs hosted in this cluster will print
# debug logs to browser console.
guiDebugMode: false

# Extra environmental variables that will be injected
# into oneprovider container.
envs: []

# Number of nodes (pod replicas) for of this provider
# their indexes will be assigned FROM 0 (ZERO!)
# up to, but not including the count value
oneprovider_nodes_count: 1

# Assign oneprovider services to oneprovider instances
# you can use values form the rage <0,oneprovider_nodes_count)
# by default the node with the highest index (oneprovider_nodes_count-1)
# is configured as a mainNode
# If a service list is empty, all available nodes are assigned that service.
cluster_config:
  managers: [ ]
  workers: [ ]
  databases: [ ]

# Oneprovider entrypoint waits for external rsync to finish updating
# container files before it continues
deployFromSources:
  enabled: false

# Erlang configuration file, which values will override
# respected default values in the app.conf of Onepanel
# The idea is to override as little as possible and allow
# the app.conf evolve on it's own.
panelOverlayConfig: |-
  [].

# Same story as above, just for worker service.
workerOverlayConfig: |-
  [].

# A map of Erlang configuration files that override
# respected default values in the app.conf of Onepanel.
# Example:
# workerExtraOverlayConfigs:
#   grafana.conf: |-
#    [].
workerExtraOverlayConfigs: []

# Same story as above, just for worker service.
panelExtraOverlayConfigs: []

# The generalization of nodeSelector.
# Allows for more fine grained controls over which
# nodes are selected by a kubernetes scheduler
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

# List of taints which are tolerated by the pods
# when nodes are selected by a kubernetes scheduler
# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: {}

# Specify a map of key-value pairs. For the pod
# to be eligible to run on a node, the node
# must have each of the indicated key-value pairs as labels
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}

privileged: false

# Generate certificates to access this onezone having trusted https connection
# You need to add cluster root certificate to your system for this to work
generate-certificates:
  enabled: false
  image: onedata/certificate-init-container:ga17ad4f
  imagePullPolicy: IfNotPresent
  signerName: clusterissuers.cert-manager.io/onedata-ci-cluster-tls
  signerNamespace: cert-manager
  signerSecret: onedata-ci-cluster-tls

# Mount certificates from a secret created before deployment
# Secret should be named in a same way, as the automatically created secret
# from generate-certificates option
certificates-from-a-secret:
  enabled: false

# Sending monitoring to Grafana/Graphite
graphite:
  enabled: false
  host: go-carbon.mon.svc.dev.onedata.uk.to
  # Depending on grafana configuration it's not necessary
  apiKey: ""
  # Template prefix, with values evaluated in runtime
  graphitePrefix: exper.${experiment}.date.${date}.args.${args}.release.${release}.dc.${dc}.cluster.${cluster}.host.${NODE_NAME}.pod.${NAMESPACE}-${POD_NAME}.container.oneprovider.uid.0

  # Static variables for a graphitePrefix
  prefixVariables:
    experiment: r19021
    # date: # computed by helm in the format 2018-02-21_11:41:14
    # args computed in the pod's entry point
    release: "r19021"
    dc: cyf
    cluster: k8s
    # host from fieldPath
    # k8s_pod from fieldPath
    # computation: oneprovider

# Create a PVC and persist /volumes/persistence
# Atm. PVC expects to find a matching PV, which is assumed is
# to be local persistent volume. Those should be created and managed using this:
# https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume/helm
persistence:
  enabled: false
  size: 20Gi
  accessModes:
    - ReadWriteOnce
  cleanUpJob:
    enabled: true
    image: groundnuty/k8s-wait-for:v2.0
    imagePullPolicy: IfNotPresent

# If set true, oneprovider will not be deployed on the same node as other oneproviders
# or onezones that are part of this deployment
onedata_anti_affinity_enabled: false

# Those credentials will be used by all onepanel REST operations etc.
onepanel_emergency_account: &onepanel_emergency_account
  name: onepanel
  password: password

# List of oneprovider onepanel users with administrative privileges
onepanel_admin_users:
  - name: admin
    password: password
  - *onepanel_emergency_account

# List of oneprovider onepanel regular users
onepanel_users:
  - name: user
    password: password

# Create jupyter notebook with oneclient-fs that will connect to this oneprovider
jupyter-notebook:
  enabled: false
  onezone_service_url: *onezone_service_url
  suffix: *providerSuffix

# Create onedata-cli instance that will have a context of this oneprovider
# and this provider onezone
onedata-cli:
  enabled: false
  suffix: *providerSuffix
  onezone_service_url: *onezone_service_url

openfaas:
  enabled: false
  suffix: *providerSuffix
  serviceType: ClusterIP
  functionNamespace: ""
  generateBasicAuth: false
  gateway:
    image: onedata/openfaas-gateway:0.26.3
    readTimeout : "14400s"
    writeTimeout : "14400s"
    upstreamTimeout : "14000s"
  basic_auth: true
  basicAuthPassword: password
  operator:
    image: onedata/openfaas-faas-netes:0.16.4
  queueWorker:
    image: onedata/openfaas-queue-worker:0.13.0
    ackWait: "14000s"
    maxInflight: 5
  faasnetes:
    image: onedata/openfaas-faas-netes:0.16.4
    readTimeout : "14400s"
    writeTimeout : "14400s"
    readinessProbe:
      initialDelaySeconds: 1
      timeoutSeconds: 20
      periodSeconds: 1
  prometheus:
    extraFlags:
      - --storage.tsdb.retention.time=1d

openfaas-pod-status-monitor:
  enabled: false
  suffix: *providerSuffix
  secret: password

# Luma configuration shared among all the storages
luma:
  enabled: false
  suffix: *providerSuffix
  lumaCacheTimeout: 5
  lumaApiKey: example_api_key
  # Enable luma for a storage type
  # atm. luma only supports posix
  posix:
    enabled: &luma_enabled_posix false
  s3:
    enabled: &luma_enabled_s3 false
  ceph:
    enabled: &luma_enabled_ceph false
  swift:
    enabled: &luma_enabled_swift false
  gluster:
    enabled: &luma_enabled_gluster false
  nfs:
    enabled: &luma_enabled_nfs false
  webdav:
    enabled: &luma_enabled_webdav false

# Create a extra containers in the same pod as Oneprovider with a dataset
# that can be imported using data sync space support option.
# Will be added to the Oneprovider as POSIX storage
# the mount points are created in /volumes/<storage_name>
# eg. /volumes/volume-data-sync-rw
volume-data-sync:
  enabled: false
  volumes:
  - name: volume-data-sync-rw
    luma-enabled: *luma_enabled_posix
    image: onedata/eo-data-mock:Landsat-5-sample-latest
    imagePullPolicy: IfNotPresent
    dataPath: /data
    readOnly: false
    mountPermissions: 777
    permissions:
      - path: Landsat-5
        # chown -R command parameters
        # the root of the path is dataPath
        user: 40001 #admin
        group: 42001 #alpha
      - path: Landsat-5/TM/L1T/2010/06/13
        user: 40001 #user
        group: 42001 #beta
      - path: Landsat-5/TM/L1T/2010/06/21
        user: 40001 #user
        group: 42001 #gamma

  - name: volume-data-sync-ro
    luma-enabled: *luma_enabled_posix
    image: onedata/eo-data-mock:Landsat-5-sample-latest
    imagePullPolicy: IfNotPresent
    dataPath: /data
    readOnly: true

# Posix local storage, it will be automatically disable if you deploy multi-node oneprovider installation
posix:
  enabled: true
  storagePath: /volumes/posix
  luma-enabled: *luma_enabled_posix
  readOnly: false
  persistence:
    name: posix
    enabled: false
    accessModes:
      - ReadWriteOnce
    size: 1Gi

# Create S3 storage server and connect with oneprovider
volume-s3:
  enabled: false
  suffix: *providerSuffix
  insecure: true
  luma-enabled: *luma_enabled_s3
  readOnly: false

# Create CEPH storage server and connect with oneprovider
volume-ceph:
  enabled: false
  suffix: *providerSuffix
  readOnly: false
  insecure: true
  luma-enabled: *luma_enabled_ceph

# Create NFS storage server and connect with oneprovider with tested default values
volume-nfs:
  enabled: false
  suffix: *providerSuffix
  exports: &nfs-exports
  - name: empty
    luma-enabled: *luma_enabled_nfs
    readOnly: false
    storageClaim: 1T
    accessModes:
      - ReadWriteMany
  mount_options:
    - soft
    - intr
    - timeo=5
    - retrans=2
  deployAsPreInstallHook: true
  cleanUpJob:
    enabled: true
  readOnly: false

# Create NFS storage server and connect with oneprovider using native nfs driver
volume-nfs-native:
  enabled: false
  suffix: *providerSuffix
  hostname: "" # generates default nfs server address
  version: "3"
  connectionPoolSize: 10
  dirCache: true
  readAhead: 0
  autoReconnect: "1"
  storagePathType: "canonical"
  exports: &nfs-exports
  - name: empty
    luma-enabled: *luma_enabled_nfs
    readOnly: false

# Create gluster storage server and connect with oneprovider with tested default values
volume-gluster:
  enabled: false
  suffix: *providerSuffix
  readOnly: false
  insecure: true
  luma-enabled: *luma_enabled_gluster

volume-swift:
  enabled: false
  suffix: *providerSuffix
  tenantName: test
  username: tester
  password: testing
  containerName: test
  readOnly: false
  insecure: true
  luma-enabled: *luma_enabled_swift

volume-webdav:
  enabled: false
  suffix: *providerSuffix
  username: tester
  password: testing
  credentialsType: basic
  rangeWriteSupport: sabredav
  verifyServerCertificate: false
  readOnly: false
  insecure: true
  luma-enabled: *luma_enabled_webdav

volume-dcache:
  enabled: false
  suffix: *providerSuffix
  username: ""
  password: ""
  credentialsType: none
  rangeWriteSupport: none
  verifyServerCertificate: false
  readOnly: true
  insecure: true
  luma-enabled: *luma_enabled_webdav

# Create oneclient instance that will connect to this oneprovider
oneclient:
  enabled: false
  onezone_service_url: *onezone_service_url
  suffix: *providerSuffix
  directIO:
    nfs:
      enable: true
      mounts: *nfs-exports
  token_dispenser_service_url:
    type: auto-generate
    disableSuffix: true

# Resultstreamer configuration that is injected to openfaas configuration
openfaas-lambda-result-streamer:
  image: onedata/openfaas-lambda-result-streamer:21.02.3

# This section will be added the end of 'storages' section in oneprovider
# cluster configuration
# avoid storage names: posix, nfs, ceph, s3, gluster, swift
external-storages: {}

# Add additional volume mounts definition to oneprovider container
external-volume-mounts: {}

# Add additional volume mounts definition
external-volumes: {}

# Add additional persistent volumes claim templates definition
external-persistent-volume-claims-templates: {}
